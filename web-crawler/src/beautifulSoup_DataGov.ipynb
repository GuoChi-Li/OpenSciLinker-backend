{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = datetime.date.today().year\n",
    "month = datetime.date.today().month\n",
    "day = datetime.date.today().day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setURL(url, num_page=-1):\n",
    "    if num_page == -1:\n",
    "        response = requests.get(str(url))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup\n",
    "    else:\n",
    "        response = requests.get(str(url) + str(num_page))\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find the project title, description and URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_page = 13128 # All pages\n",
    "# num_page = 11 # for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for page_index in range(1, num_page):\n",
    "#     soup = setURL('https://catalog.data.gov/dataset/?page=', page_index)\n",
    "\n",
    "#     item_len = len(soup.find_all('li', class_=\"dataset-item has-organization\"))\n",
    "\n",
    "#     for index in range(item_len):\n",
    "#         print(index)\n",
    "#         all_result = soup.find_all('div', class_='dataset-content')\n",
    "#         each_url = 'https://catalog.data.gov/' + str(all_result[index].find('a').get('href'))\n",
    "#         print(each_url)\n",
    "\n",
    "#         into_each_result = setURL(each_url, -1)\n",
    "\n",
    "#         # print(into_each_result)\n",
    "#         title = into_each_result.find('h1', itemprop=\"name\").text.strip()\n",
    "#         description = into_each_result.find('div', itemprop=\"description\").text.strip()\n",
    "#         updated_date = into_each_result.find('a', href=\"#sec-dates\").text.strip()\n",
    "#         created_date = into_each_result.find(id=\"sec-dates\").find('td').text.strip()\n",
    "#         publisher = into_each_result.find('span', itemprop=\"name\").text.strip()\n",
    "#         print(title)\n",
    "#         print(description)\n",
    "#         print(publisher)\n",
    "#         print(created_date)\n",
    "#         print(updated_date)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = []\n",
    "\n",
    "def write_json(json_data, page_index):\n",
    "    json_data = json.dumps(json_data, indent=4)  # 'indent' for pretty formatting\n",
    "\n",
    "    # Specify the file path where you want to save the JSON data\n",
    "    file_path = \"./../data/Data_Gov/\" + str(year) + str(month) + str(day) + \"_data_\" + str(page_index)+ \"Pages\" + \".json\"\n",
    "\n",
    "    # Write the JSON string to the file\n",
    "    with open(file_path, 'w', encoding='utf-8') as json_file:\n",
    "        json_file.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_save=10\n",
    "\n",
    "for page_index in range(1, num_page):\n",
    "    print('current page: ', page_index)\n",
    "    soup = setURL('https://catalog.data.gov/dataset/?page=', page_index)\n",
    "    item_len = len(soup.find_all('li', class_=\"dataset-item has-organization\"))\n",
    "\n",
    "    for index in range(item_len):\n",
    "\n",
    "        all_result = soup.find_all('div', class_='dataset-content')\n",
    "        each_url = 'https://catalog.data.gov' + str(all_result[index].find('a').get('href'))\n",
    "\n",
    "        into_each_result = setURL(each_url, -1)\n",
    "        try:\n",
    "            title = into_each_result.find('h1', itemprop=\"name\").text.strip()\n",
    "            description = into_each_result.find('div', itemprop=\"description\").text.strip()\n",
    "            updated_date = into_each_result.find('a', href=\"#sec-dates\").text.strip()\n",
    "            created_date = into_each_result.find(id=\"sec-dates\").find('td').text.strip()\n",
    "            publisher = into_each_result.find('span', itemprop=\"name\").text.strip()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        json_data.append({\"title\":title,'Project URL':each_url, 'description':description, 'publisher':publisher, 'updated_date':updated_date, 'created_date':created_date})\n",
    "\n",
    "        if page_index % num_save == 0:\n",
    "            write_json(json_data, page_index)\n",
    "            \n",
    "write_json(json_data, num_page)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebCrawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
